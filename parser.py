import re
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
# –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
CHANNELS = [
    "https://t.me/s/Zaporizhzhyaoblenergo_news",  # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π
    "https://t.me/s/info_zp"                      # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–Ω–∏ —Å–ª–æ–≤ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
# –ì–ü–í - —Å—Ç–∞–Ω–¥–∞—Ä—Ç
# –ì–†–ê–§–Ü–ö - –ª–æ–≤–∏—Ç "–≥—Ä–∞—Ñ—ñ–∫", "–≥—Ä–∞—Ñ—ñ–∫—É", "–≥—Ä–∞—Ñ—ñ–∫–∏"
# –í–Ü–î–ö–õ–Æ–ß–ï–ù - –ª–æ–≤–∏—Ç "–≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è", "–≤—ñ–¥–∫–ª—é—á–µ–Ω–æ"
# –ï–õ–ï–ö–¢–†–û - –ª–æ–≤–∏—Ç "–µ–ª–µ–∫—Ç—Ä–æ–ø–æ—Å—Ç–∞—á–∞–Ω–Ω—è", "–µ–ª–µ–∫—Ç—Ä–æ–µ–Ω–µ—Ä–≥—ñ—è"
# –ß–ï–†–ì - –ª–æ–≤–∏—Ç "—á–µ—Ä–≥–∞", "—á–µ—Ä–≥–∏", "—á–µ—Ä–≥–∞–º", "–ø–æ —á–µ—Ä–≥–∞—Ö"
# –û–ù–û–í–õ–ï–ù - –ª–æ–≤–∏—Ç "–æ–Ω–æ–≤–ª–µ–Ω–æ", "–æ–Ω–æ–≤–ª–µ–Ω–Ω—è" (–≤–∞–∂–Ω–æ –¥–ª—è info_zp)
# –ó–ú–Ü–ù - –ª–æ–≤–∏—Ç "–∑–º—ñ–Ω–∏", "–∑–º—ñ–Ω–µ–Ω–æ"
# –û–ë–õ–ï–ù–ï–†–ì–û - —á–∞—Å—Ç–æ –ø–∏—à—É—Ç –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
KEYWORDS = [
    "–ì–ü–í", "–ì–†–ê–§–Ü–ö", "–í–Ü–î–ö–õ–Æ–ß–ï–ù", "–ï–õ–ï–ö–¢–†–û", "–ß–ï–†–ì", 
    "–û–ù–û–í–õ–ï–ù", "–ó–ú–Ü–ù", "–û–ë–õ–ï–ù–ï–†–ì–û", "–£–ö–†–ï–ù–ï–†–ì–û", "–°–í–Ü–¢–õ"
]

# –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Å—è—Ü–µ–≤
UA_MONTHS = {
    "–°–Ü–ß–ù–Ø": 1, "–õ–Æ–¢–û–ì–û": 2, "–ë–ï–†–ï–ó–ù–Ø": 3, "–ö–í–Ü–¢–ù–Ø": 4, "–¢–†–ê–í–ù–Ø": 5, "–ß–ï–†–í–ù–Ø": 6,
    "–õ–ò–ü–ù–Ø": 7, "–°–ï–†–ü–ù–Ø": 8, "–í–ï–†–ï–°–ù–Ø": 9, "–ñ–û–í–¢–ù–Ø": 10, "–õ–ò–°–¢–û–ü–ê–î–ê": 11, "–ì–†–£–î–ù–Ø": 12
}
# –û–±—Ä–∞—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ (–ß–∏—Å–ª–æ -> –ù–∞–∑–≤–∞–Ω–∏–µ) –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª—é—á–∞ –¥–∞—Ç—ã –∏–∑ timestamp
UA_MONTHS_REVERSE = {v: k for k, v in UA_MONTHS.items()}

def get_kiev_time():
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è UTC –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç 2 —á–∞—Å–∞ (–∑–∏–º–Ω–µ–µ –≤—Ä–µ–º—è)"""
    return datetime.utcnow() + timedelta(hours=2)

def get_html(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            return response.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
    return None

def parse_channel(url):
    html = get_html(url)
    if not html: return []

    soup = BeautifulSoup(html, 'html.parser')
    message_wraps = soup.find_all('div', class_='tgme_widget_message_wrap')
    
    found_schedules = []

    # –†–µ–≥—É–ª—è—Ä–∫–∏
    months_regex = "|".join(UA_MONTHS.keys())
    # –ò—â–µ–º –¥–∞—Ç—É (—á–∏—Å–ª–æ + –º–µ—Å—è—Ü)
    date_pattern = re.compile(rf"(\d{{1,2}})\s+({months_regex})", re.IGNORECASE)
    # –ò—â–µ–º –æ—á–µ—Ä–µ–¥–∏ (1.1: –≤—Ä–µ–º—è)
    queue_pattern = re.compile(r"^(\d\.\d)\s*[:]\s*(.*)")
    # –ò—â–µ–º –≤—Ä–µ–º—è (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–µ—Ñ–∏—Å–∞, —Ç–∏—Ä–µ, –º–∏–Ω—É—Å–∞)
    time_pattern = re.compile(r"(\d{1,2}:\d{2})\s*[-‚Äì‚Äî‚àí]\s*(\d{1,2}:\d{2})")

    for wrap in reversed(message_wraps):
        text_div = wrap.find('div', class_='tgme_widget_message_text')
        if not text_div: continue
        text = text_div.get_text(separator="\n")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        # –¢–µ–ø–µ—Ä—å –∏—â–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ—Ä–Ω–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä "–û–ù–û–í–õ–ï–ù" –Ω–∞–π–¥–µ—Ç –≤ "–û–Ω–æ–≤–ª–µ–Ω–Ω—è")
        if not any(k in text.upper() for k in KEYWORDS):
            continue

        # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (timestamp)
        time_tag = wrap.find('time')
        post_timestamp = ""
        if time_tag and time_tag.has_attr('datetime'):
            post_timestamp = time_tag['datetime']
        else:
            continue

        lines = [line.strip().replace('\xa0', ' ') for line in text.split('\n') if line.strip()]
        
        explicit_date_key = None
        updated_at_val = None
        queues_found = {}

        # 1. –°–Ω–∞—á–∞–ª–∞ –ø–∞—Ä—Å–∏–º –æ—á–µ—Ä–µ–¥–∏ –∏ –∏—â–µ–º —è–≤–Ω—É—é –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ
        for line in lines:
            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä "13 –ì–†–£–î–ù–Ø")
            if not explicit_date_key:
                match = date_pattern.search(line)
                if match:
                    day, month = match.groups()
                    explicit_date_key = f"{day} {month.upper()}"

            # –ò—â–µ–º –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–û–Ω–æ–≤–ª–µ–Ω–æ –æ ...)
            if not updated_at_val:
                time_upd_match = re.search(r"\(–æ–Ω–æ–≤–ª–µ–Ω–æ.*(\d{2}:\d{2})\)", line, re.IGNORECASE)
                if time_upd_match:
                    updated_at_val = time_upd_match.group(1)

            # –ü–∞—Ä—Å–∏–º –æ—á–µ—Ä–µ–¥–∏
            q_match = queue_pattern.search(line)
            if q_match:
                q_id = q_match.group(1)
                times_raw = q_match.group(2)
                
                intervals = []
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ —Ç–æ—á–∫–µ —Å –∑–∞–ø—è—Ç–æ–π
                parts = re.split(r"[,;]", times_raw)
                for part in parts:
                    t_match = time_pattern.search(part)
                    if t_match:
                        start, end = t_match.groups()
                        intervals.append({"start": start, "end": end})
                
                if intervals:
                    queues_found[q_id] = intervals

        # 2. –ï—Å–ª–∏ –º—ã –Ω–∞—à–ª–∏ –æ—á–µ—Ä–µ–¥–∏ (–ì–õ–ê–í–ù–û–ï –£–°–õ–û–í–ò–ï), —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        # –î–∞–∂–µ –µ—Å–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –±—ã–ª–æ "–í–∏ –Ω–µ –ø–æ–≤—ñ—Ä–∏—Ç–µ", –Ω–æ –≤–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å "1.1: 00-04", –º—ã —ç—Ç–æ –±–µ—Ä–µ–º.
        if queues_found:
            final_date_key = None

            if explicit_date_key:
                # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –±—ã–ª–∞ –¥–∞—Ç–∞ - –±–µ—Ä–µ–º –µ—ë
                final_date_key = explicit_date_key
            else:
                # –§–û–õ–õ–ë–≠–ö: –ï—Å–ª–∏ –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç (–ø–æ—Å—Ç—ã —Ç–∏–ø–∞ "–û–Ω–æ–≤–ª–µ–Ω–æ –≥—Ä–∞—Ñ—ñ–∫"),
                # –±–µ—Ä–µ–º –¥–∞—Ç—É –∏–∑ timestamp —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–æ –ö–∏–µ–≤—Å–∫–æ–º—É –≤—Ä–µ–º–µ–Ω–∏)
                try:
                    dt = datetime.fromisoformat(post_timestamp.replace('Z', '+00:00'))
                    dt_kiev = dt + timedelta(hours=2)
                    day = dt_kiev.day
                    month_name = UA_MONTHS_REVERSE.get(dt_kiev.month, "–ì–†–£–î–ù–Ø")
                    final_date_key = f"{day} {month_name}"
                except Exception as e:
                    print(f"‚ö†Ô∏è Date fallback error: {e}")
                    continue

            # –ï—Å–ª–∏ –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –±–µ—Ä–µ–º –≤—Ä–µ–º—è –ø–æ—Å—Ç–∞
            if not updated_at_val:
                try:
                    dt = datetime.fromisoformat(post_timestamp.replace('Z', '+00:00'))
                    dt_kiev = dt + timedelta(hours=2)
                    updated_at_val = dt_kiev.strftime("%H:%M")
                except:
                    updated_at_val = "??:??"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            found_schedules.append({
                "date": final_date_key,
                "queues": queues_found,
                "updated_at": updated_at_val,
                "source_ts": post_timestamp
            })

    return found_schedules

def merge_schedules(all_schedules):
    merged = {}
    for sch in all_schedules:
        d_key = sch['date']
        if d_key not in merged:
            merged[d_key] = sch
        else:
            # –ï—Å–ª–∏ –¥–∞—Ç–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –±–µ—Ä–µ–º –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–π –ø–æ—Å—Ç
            existing_ts = merged[d_key]['source_ts']
            new_ts = sch['source_ts']
            if new_ts > existing_ts:
                print(f"üîÑ Updated {d_key} from newer post.")
                merged[d_key] = sch
    return list(merged.values())

def main():
    all_found = []
    
    for url in CHANNELS:
        print(f"üì° Parsing {url}...")
        res = parse_channel(url)
        print(f"   Found {len(res)} schedules.")
        all_found.extend(res)

    final_list = merge_schedules(all_found)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    def date_sorter(item):
        try:
            parts = item['date'].split()
            day = int(parts[0])
            month_str = parts[1]
            month = UA_MONTHS.get(month_str, 0)
            now = datetime.now()
            year = now.year
            # –ü–µ—Ä–µ—Ö–æ–¥ –≥–æ–¥–∞ (–¥–µ–∫–∞–±—Ä—å -> —è–Ω–≤–∞—Ä—å)
            if now.month == 12 and month == 1:
                year += 1
            return datetime(year, month, day)
        except:
            return datetime.now()

    final_list.sort(key=date_sorter)
    final_list = final_list[-3:]

    output_json = {
        "last_check": get_kiev_time().strftime("%d.%m %H:%M"),
        "schedules": final_list
    }

    # –ß–∏—Å—Ç–∏–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
    for item in output_json["schedules"]:
        if "source_ts" in item:
            del item["source_ts"]

    with open('schedule.json', 'w', encoding='utf-8') as f:
        json.dump(output_json, f, ensure_ascii=False, indent=4)
        
    print(f"üíæ Saved {len(final_list)} schedules.")

if __name__ == "__main__":
    main()
