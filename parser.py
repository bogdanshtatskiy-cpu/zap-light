import re
import json
import requests
import socket
import time
import sys
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote
import requests.packages.urllib3.util.connection as urllib3_cn

# ==========================
# üîß –§–ò–ö–° –î–õ–Ø GITHUB ACTIONS (IPv4)
# ==========================
def allowed_gai_family():
    return socket.AF_INET

urllib3_cn.allowed_gai_family = allowed_gai_family

# ==========================
# ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================

# –°—Å—ã–ª–∫–∞ —Å /s/ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –≤–µ–±-–≤–µ—Ä—Å–∏–∏
CHANNELS = [
    "https://t.me/s/zapnovini"
]

KEYWORDS = [
    "–ì–ü–í", "–ì–†–ê–§–Ü–ö", "–í–Ü–î–ö–õ–Æ–ß–ï–ù", "–ï–õ–ï–ö–¢–†–û", "–ß–ï–†–ì", 
    "–û–ù–û–í–õ–ï–ù", "–ó–ú–Ü–ù", "–û–ë–õ–ï–ù–ï–†–ì–û", "–£–ö–†–ï–ù–ï–†–ì–û", "–°–í–Ü–¢–õ"
]

UA_MONTHS = {
    "–°–Ü–ß–ù–Ø": 1, "–õ–Æ–¢–û–ì–û": 2, "–ë–ï–†–ï–ó–ù–Ø": 3, "–ö–í–Ü–¢–ù–Ø": 4, "–¢–†–ê–í–ù–Ø": 5, "–ß–ï–†–í–ù–Ø": 6,
    "–õ–ò–ü–ù–Ø": 7, "–°–ï–†–ü–ù–Ø": 8, "–í–ï–†–ï–°–ù–Ø": 9, "–ñ–û–í–¢–ù–Ø": 10, "–õ–ò–°–¢–û–ü–ê–î–ê": 11, "–ì–†–£–î–ù–Ø": 12
}
UA_MONTHS_REVERSE = {v: k for k, v in UA_MONTHS.items()}

# ==========================
# üõ† –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================

def get_kiev_time():
    return datetime.utcnow() + timedelta(hours=2)

def log(msg):
    print(msg)
    sys.stdout.flush()

def get_html(target_url):
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã Telegram
    proxies = [
        f"https://api.allorigins.win/raw?url={quote(target_url)}",
        f"https://corsproxy.io/?{quote(target_url)}",
        f"https://api.codetabs.com/v1/proxy?quest={quote(target_url)}"
    ]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Cache-Control': 'no-cache'
    }

    for url in proxies:
        try:
            log(f"   üîÑ –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑: {url[:40]}...")
            response = requests.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200 and len(response.text) > 2000:
                log(f"   ‚úÖ –°–∫–∞—á–∞–Ω–æ {len(response.text)} –±–∞–π—Ç.")
                return response.text
            else:
                log(f"   ‚ö†Ô∏è –ù–µ—É–¥–∞—á–Ω–æ (–ö–æ–¥: {response.status_code}, –†–∞–∑–º–µ—Ä: {len(response.text)})")
                
        except Exception as e:
            log(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)[:50]}...")
        
        time.sleep(1)

    return None

def parse_channel(url):
    html = get_html(url)
    if not html: return []

    soup = BeautifulSoup(html, 'html.parser')
    page_title = soup.title.string.strip() if soup.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    log(f"   üîé –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: '{page_title}'")
    
    # –ò—â–µ–º –±–ª–æ–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    message_divs = soup.find_all('div', class_='tgme_widget_message_text')
    if not message_divs:
        message_divs = soup.find_all('div', class_='js-message_text')
        
    log(f"   üîé –ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(message_divs)}")
    
    if len(message_divs) == 0:
        return []

    found_schedules = []
    
    # –†–µ–≥—É–ª—è—Ä–∫–∏
    months_regex = "|".join(UA_MONTHS.keys())
    date_pattern = re.compile(rf"(\d{{1,2}})\s+({months_regex})", re.IGNORECASE)
    # –í—Ä–µ–º—è: "00:00 - 02:00" (—É—á—Ç–µ–Ω—ã —Ä–∞–∑–Ω—ã–µ —Ç–∏—Ä–µ)
    time_pattern = re.compile(r"(\d{1,2}[:.]\d{2})\s*[-‚Äì‚Äî‚àí]\s*(\d{1,2}[:.]\d{2})")
    # –û—á–µ—Ä–µ–¥—å –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏: "1.1: ..."
    direct_queue_pattern = re.compile(r"^(\d\.\d)\s*[:]\s*(.*)")
    # –û—á–µ—Ä–µ–¥—å –≥–¥–µ-—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
    specific_queue_pattern = re.compile(r"\b([1-6]\.[12])\b")

    for text_div in message_divs:
        text = text_div.get_text(separator="\n")

        if not any(k in text.upper() for k in KEYWORDS):
            continue

        updated_at_val = None
        lines = [line.strip().replace('\xa0', ' ') for line in text.split('\n') if line.strip()]
        
        explicit_date_key = None
        queues_found = {}

        for line in lines:
            # 1. –ü–æ–∏—Å–∫ –¥–∞—Ç—ã (–Ω–∞–ø—Ä. "8 –°–Ü–ß–ù–Ø")
            if not explicit_date_key:
                match = date_pattern.search(line)
                if match:
                    day_raw, month = match.groups()
                    day_clean = str(int(day_raw))
                    explicit_date_key = f"{day_clean} {month.upper()}"

            # 2. –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å—Ç–∞
            if not updated_at_val:
                time_upd_match = re.search(r"\(–æ–Ω–æ–≤–ª–µ–Ω–æ.*(\d{2}:\d{2})\)", line, re.IGNORECASE)
                if time_upd_match:
                    updated_at_val = time_upd_match.group(1)

            # 3. –ê–ù–ê–õ–ò–ó –°–¢–†–û–ö–ò –° –û–ß–ï–†–ï–î–¨–Æ
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç "1.1: –≤—Ä–µ–º—è..."
            queue_match = direct_queue_pattern.search(line)
            
            if queue_match:
                # –ù–∞—à–ª–∏ —è–≤–Ω–æ–µ –Ω–∞—á–∞–ª–æ "1.1:"
                q_id = queue_match.group(1)
                content = queue_match.group(2)
                
                intervals = []
                time_matches = list(time_pattern.finditer(content))
                for tm in time_matches:
                    start, end = tm.groups()
                    start = start.replace('.', ':')
                    end = end.replace('.', ':')
                    if len(start) == 4: start = "0" + start
                    if len(end) == 4: end = "0" + end
                    intervals.append({"start": start, "end": end})
                
                # –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –Ω–µ—Ç, –Ω–æ —Å—Ç—Ä–æ–∫–∞ –µ—Å—Ç—å (–Ω–∞–ø—Ä. "4.2: –Ω–µ –≤–∏–º–∏–∫–∞—î—Ç—å—Å—è") -> —ç—Ç–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ (—Å–≤–µ—Ç –µ—Å—Ç—å)
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ intervals, —ç—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç—Å—è –∫–∞–∫ "–°–≤–µ—Ç –µ—Å—Ç—å"
                queues_found[q_id] = intervals
                
            else:
                # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞: –∏—â–µ–º –≤—Ä–µ–º—è, –∞ –ø–µ—Ä–µ–¥ –Ω–∏–º –æ—á–µ—Ä–µ–¥—å
                time_matches = list(time_pattern.finditer(line))
                if time_matches:
                    intervals = []
                    for tm in time_matches:
                        start, end = tm.groups()
                        start = start.replace('.', ':')
                        end = end.replace('.', ':')
                        if len(start) == 4: start = "0" + start
                        if len(end) == 4: end = "0" + end
                        intervals.append({"start": start, "end": end})

                    text_before_time = line[:time_matches[0].start()]
                    found_sub_queues = specific_queue_pattern.findall(text_before_time)
                    
                    for q_id in found_sub_queues:
                        if q_id not in queues_found:
                            queues_found[q_id] = []
                        queues_found[q_id].extend(intervals)

        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ—Å—Ç–∞
        if queues_found:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –æ—á–µ—Ä–µ–¥–µ–π
            for q_id in queues_found:
                # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
                unique = []
                seen = set()
                for i in queues_found[q_id]:
                    key = f"{i['start']}-{i['end']}"
                    if key not in seen:
                        seen.add(key)
                        unique.append(i)
                unique.sort(key=lambda x: x['start'])
                queues_found[q_id] = unique

            final_date_key = None

            if explicit_date_key:
                final_date_key = explicit_date_key
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è —É–≥–∞–¥–∞—Ç—å –¥–∞—Ç—É –ø–æ "–∑–∞–≤—Ç—Ä–∞"/"—Å–µ–≥–æ–¥–Ω—è"
                now_kiev = get_kiev_time()
                if "–∑–∞–≤—Ç—Ä–∞" in text.lower():
                    target_date = now_kiev + timedelta(days=1)
                else:
                    target_date = now_kiev
                
                day = target_date.day
                month_name = UA_MONTHS_REVERSE.get(target_date.month, "–ì–†–£–î–ù–Ø")
                final_date_key = f"{day} {month_name}"

            if not updated_at_val:
                updated_at_val = get_kiev_time().strftime("%H:%M")

            log(f"   ‚ûï –ù–∞–π–¥–µ–Ω –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ {final_date_key} (–æ—á–µ—Ä–µ–¥–µ–π: {len(queues_found)})")
            
            found_schedules.append({
                "date": final_date_key,
                "queues": queues_found,
                "updated_at": updated_at_val
            })

    return found_schedules

# ==========================
# üíæ –õ–û–ì–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø
# ==========================

def load_existing_schedules():
    if os.path.exists('schedule.json'):
        try:
            with open('schedule.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("schedules", [])
        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
    return []

def merge_schedules(old_data, new_data):
    merged = {}
    # –°–Ω–∞—á–∞–ª–∞ —Å—Ç–∞—Ä—ã–µ
    for sch in old_data:
        merged[sch['date']] = sch
    # –ü–æ—Ç–æ–º –Ω–æ–≤—ã–µ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç —Å—Ç–∞—Ä—ã–µ)
    for sch in new_data:
        merged[sch['date']] = sch
    return list(merged.values())

def main():
    old_schedules = load_existing_schedules()
    log(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(old_schedules)}")

    new_found = []
    for url in CHANNELS:
        log(f"üì° –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–Ω–∞–ª–∞: {url}")
        res = parse_channel(url)
        if res:
            log(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏: {len(res)}")
            new_found.extend(res)
        else:
            log("   ‚ùå –ì—Ä–∞—Ñ–∏–∫–æ–≤ –≤ –ø–æ—Å—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ –Ω–æ–≤–æ–≥–æ, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º —Ñ–∞–π–ª, —á—Ç–æ–±—ã –Ω–µ —Å—Ç–µ—Ä–µ—Ç—å —Å—Ç–∞—Ä–æ–µ
    if not new_found:
        log("‚ö†Ô∏è –ù–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç. –§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–µ–Ω.")
        return

    final_list = merge_schedules(old_schedules, new_found)

    def date_sorter(item):
        try:
            parts = item['date'].split()
            day = int(parts[0])
            month_str = parts[1]
            month = UA_MONTHS.get(month_str, 0)
            now = datetime.now()
            year = now.year
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ–Ω—ã –≥–æ–¥–∞ (–¥–µ–∫–∞–±—Ä—å -> —è–Ω–≤–∞—Ä—å)
            if now.month == 12 and month == 1: year += 1
            elif now.month == 1 and month == 12: year -= 1
            return datetime(year, month, day)
        except:
            return datetime.now()

    final_list.sort(key=date_sorter)
    final_list = final_list[-3:] # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω—è

    output_json = {
        "last_check": get_kiev_time().strftime("%d.%m %H:%M"),
        "schedules": final_list
    }

    with open('schedule.json', 'w', encoding='utf-8') as f:
        json.dump(output_json, f, ensure_ascii=False, indent=4)
        
    dates_in_file = [item['date'] for item in final_list]
    log(f"üíæ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ! –î–∞—Ç—ã –≤ —Ñ–∞–π–ª–µ: {dates_in_file}")

if __name__ == "__main__":
    main()

