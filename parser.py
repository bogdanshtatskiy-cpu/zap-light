import re
import json
import requests
import socket
import time
import sys
import os
import random
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
import requests.packages.urllib3.util.connection as urllib3_cn

# ==========================
# üîß –§–ò–ö–° –î–õ–Ø GITHUB ACTIONS (IPv4)
# ==========================
def allowed_gai_family():
    return socket.AF_INET

urllib3_cn.allowed_gai_family = allowed_gai_family

# ==========================
# ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================

CHANNELS = [
    "https://t.me/s/it_is_zp_tg",
    "https://t.me/s/tvoe_zaporizhzhia",
    "https://t.me/s/zapnovini",
    "https://t.me/s/info_zp",
    "https://t.me/s/zoe_alarm"
]

KEYWORDS = [
    "–ì–ü–í", "–ì–†–ê–§–Ü–ö", "–í–Ü–î–ö–õ–Æ–ß–ï–ù", "–ï–õ–ï–ö–¢–†–û", "–ß–ï–†–ì", 
    "–û–ù–û–í–õ–ï–ù", "–ó–ú–Ü–ù", "–û–ë–õ–ï–ù–ï–†–ì–û", "–£–ö–†–ï–ù–ï–†–ì–û", "–°–í–Ü–¢–õ"
]

UA_MONTHS = {
    "–°–Ü–ß–ù–Ø": 1, "–õ–Æ–¢–û–ì–û": 2, "–ë–ï–†–ï–ó–ù–Ø": 3, "–ö–í–Ü–¢–ù–Ø": 4, "–¢–†–ê–í–ù–Ø": 5, "–ß–ï–†–í–ù–Ø": 6,
    "–õ–ò–ü–ù–Ø": 7, "–°–ï–†–ü–ù–Ø": 8, "–í–ï–†–ï–°–ù–Ø": 9, "–ñ–û–í–¢–ù–Ø": 10, "–õ–ò–°–¢–û–ü–ê–î–ê": 11, "–ì–†–£–î–ù–Ø": 12
}
UA_MONTHS_REVERSE = {v: k for k, v in UA_MONTHS.items()}

NO_OUTAGE_PHRASES = [
    "–ù–ï –í–ò–ú–ò–ö–ê–Ñ–¢–¨–°–Ø", "–ù–ï –ó–ê–°–¢–û–°–û–í–£–Æ–¢–¨–°–Ø", "–ë–ï–ó –í–Ü–î–ö–õ–Æ–ß–ï–ù–¨", 
    "–°–ö–ê–°–û–í–ê–ù–û", "–ë–Ü–õ–ò–ô", "–ó–ï–õ–ï–ù–ò–ô"
]

# ==========================
# üõ† –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================

def get_kiev_time():
    return datetime.now(timezone.utc) + timedelta(hours=2)

def log(msg):
    print(msg)
    sys.stdout.flush()

def get_html(target_url):
    rnd = random.randint(1, 999999)
    proxies = [
        f"https://api.allorigins.win/raw?url={quote(target_url)}&rnd={rnd}",
        f"https://corsproxy.io/?{quote(target_url)}", 
        f"https://api.codetabs.com/v1/proxy?quest={quote(target_url)}&rnd={rnd}"
    ]
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }
    for url in proxies:
        try:
            log(f"   üîÑ –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏...")
            response = requests.get(url, headers=headers, timeout=20)
            if response.status_code == 200 and len(response.text) > 2000:
                content = response.text
                if "tgme_widget" in content:
                    return content
        except Exception:
            pass
        time.sleep(1)
    return None

def parse_post_date(date_str):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –ø–æ—Å—Ç–∞ –∏–∑ HTML (–æ–±—ã—á–Ω–æ ISO —Ñ–æ—Ä–º–∞—Ç)"""
    try:
        # –ü—Ä–∏–º–µ—Ä: 2024-01-14T18:00:00+00:00
        dt = datetime.fromisoformat(date_str)
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –ö–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è (UTC+2 / UTC+3)
        return dt.astimezone(timezone(timedelta(hours=2)))
    except Exception:
        return get_kiev_time()

def determine_date_from_text(text, post_date):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–∞—Ç—É –≥—Ä–∞—Ñ–∏–∫–∞.
    post_date - —ç—Ç–æ datetime –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è (–ö–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è)
    """
    text_upper = text.upper()
    
    # 1. –ü–æ–∏—Å–∫ —è–≤–Ω–æ–π –¥–∞—Ç—ã (15 –°–Ü–ß–ù–Ø)
    months_regex = "|".join(UA_MONTHS.keys())
    date_match = re.search(rf"\b(\d{{1,2}})\s+({months_regex})\b", text_upper)
    if date_match:
        day = int(date_match.group(1))
        month_name = date_match.group(2)
        return f"{day} {month_name}"

    # 2. –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç—ã (–°–ß–ò–¢–ê–ï–ú –û–¢ –î–ê–¢–´ –ü–û–°–¢–ê, –ê –ù–ï –û–¢ –¢–ï–ö–£–©–ï–ô)
    if "–ó–ê–í–¢–†–ê" in text_upper:
        target_date = post_date + timedelta(days=1)
        day = target_date.day
        month_name = UA_MONTHS_REVERSE.get(target_date.month, "–ì–†–£–î–ù–Ø")
        return f"{day} {month_name}"
    
    if "–°–¨–û–ì–û–î–ù–Ü" in text_upper:
        target_date = post_date
        day = target_date.day
        month_name = UA_MONTHS_REVERSE.get(target_date.month, "–ì–†–£–î–ù–Ø")
        return f"{day} {month_name}"

    return None

def parse_channel(url):
    html = get_html(url)
    if not html: return []

    soup = BeautifulSoup(html, 'html.parser')
    page_title = soup.title.string.strip() if soup.title else "Channel"
    log(f"   üîé –ê–Ω–∞–ª–∏–∑: {page_title}")
    
    # üî• –ò—â–µ–º –±–ª–æ–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π —Ü–µ–ª–∏–∫–æ–º, —á—Ç–æ–±—ã –¥–æ—Å—Ç–∞—Ç—å –∏ –¢–ï–ö–°–¢, –∏ –î–ê–¢–£
    message_wraps = soup.find_all('div', class_='tgme_widget_message')
    
    if not message_wraps:
        # –§–æ–ª–ª–±—ç–∫ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –ø—Ä–æ–∫—Å–∏
        return []

    found_schedules = []
    
    # –†–µ–≥—É–ª—è—Ä–∫–∏
    time_pattern = re.compile(r"(?:–∑\s*)?(\d{1,2}[:.;]\d{2})\s*(?:[-‚Äì‚Äî‚àí]|–¥–æ|–ø–æ)\s*(\d{1,2}[:.;]\d{2})", re.IGNORECASE)
    queue_pattern = re.compile(r"^\s*(?:–ß–µ—Ä–≥–∞\s*)?(\d\.\d)\s*[:)]?\s*(.*)", re.IGNORECASE)

    for msg in message_wraps:
        # 1. –î–æ—Å—Ç–∞–µ–º —Ç–µ–∫—Å—Ç
        text_div = msg.find('div', class_='tgme_widget_message_text')
        if not text_div: continue
        text = text_div.get_text(separator="\n")

        if not any(k in text.upper() for k in KEYWORDS):
            continue

        # 2. –î–æ—Å—Ç–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞ (–¥–ª—è —Ñ–∏–∫—Å–∞ "–ó–ê–í–¢–†–ê")
        post_date = get_kiev_time()
        time_tag = msg.find('time')
        if time_tag and 'datetime' in time_tag.attrs:
            post_date = parse_post_date(time_tag['datetime'])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—É –≥—Ä–∞—Ñ–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç—ã –ø–æ—Å—Ç–∞
        final_date_key = determine_date_from_text(text, post_date)
        if not final_date_key:
            continue

        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        # –ë–µ—Ä–µ–º –≤—Ä–µ–º—è –∏–∑ –ø–æ—Å—Ç–∞, –µ—Å–ª–∏ –µ—Å—Ç—å "(–æ–Ω–æ–≤–ª–µ–Ω–æ 10:00)", –∏–Ω–∞—á–µ –≤—Ä–µ–º—è –ø–æ—Å—Ç–∞
        updated_at_val = post_date.strftime("%d.%m %H:%M")
        time_upd_match = re.search(r"\(–æ–Ω–æ–≤–ª–µ–Ω–æ.*(\d{2}:\d{2})\)", text, re.IGNORECASE)
        if time_upd_match:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω–æ–µ –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, –±–µ—Ä–µ–º –¥–∞—Ç—É –ø–æ—Å—Ç–∞ + —ç—Ç–æ –≤—Ä–µ–º—è
            updated_at_val = f"{post_date.strftime('%d.%m')} {time_upd_match.group(1)}"

        # –ü–∞—Ä—Å–∏–Ω–≥ –æ—á–µ—Ä–µ–¥–µ–π
        lines = [line.strip().replace('\xa0', ' ') for line in text.split('\n') if line.strip()]
        queues_found = {}

        for line in lines:
            q_match = queue_pattern.search(line)
            if q_match:
                q_id = q_match.group(1)
                content = q_match.group(2).lower()
                
                if any(phrase.lower() in content for phrase in NO_OUTAGE_PHRASES):
                    queues_found[q_id] = [] 
                    continue

                intervals = []
                time_matches = list(time_pattern.finditer(content))
                for tm in time_matches:
                    start, end = tm.groups()
                    start = start.replace('.', ':').replace(';', ':')
                    end = end.replace('.', ':').replace(';', ':')
                    if len(start) == 4: start = "0" + start
                    if len(end) == 4: end = "0" + end
                    intervals.append({"start": start, "end": end})
                
                if intervals:
                    queues_found[q_id] = intervals
                elif not intervals and len(content) < 30:
                     queues_found[q_id] = []

        if queues_found:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
            for q_id in queues_found:
                unique = []
                seen = set()
                for i in queues_found[q_id]:
                    key = f"{i['start']}-{i['end']}"
                    if key not in seen:
                        seen.add(key)
                        unique.append(i)
                unique.sort(key=lambda x: x['start'])
                queues_found[q_id] = unique

            log(f"   ‚ûï –ì—Ä–∞—Ñ–∏–∫ –Ω–∞ {final_date_key} (–∏–∑ –ø–æ—Å—Ç–∞ –æ—Ç {post_date.strftime('%d.%m %H:%M')})")
            
            found_schedules.append({
                "date": final_date_key,
                "queues": queues_found,
                "updated_at": updated_at_val
            })

    return found_schedules

def load_existing_schedules():
    if os.path.exists('schedule.json'):
        try:
            with open('schedule.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("schedules", [])
        except Exception:
            return []
    return []

def merge_schedules(old_data, new_data):
    merged = {}
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä–æ–µ
    for sch in old_data:
        merged[sch['date']] = sch
    
    # 2. –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º –Ω–æ–≤–æ–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ —Å–ø–∏—Å–∫–µ CHANNELS)
    # –í–∞–∂–Ω–æ: –µ—Å–ª–∏ new_data —Å–æ–¥–µ—Ä–∂–∏—Ç –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ 15-–µ, –æ–Ω –ø–µ—Ä–µ–∑–∞–ø–∏—à–µ—Ç —Å—Ç–∞—Ä—ã–π
    for sch in new_data:
        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –≤ –Ω–æ–≤–æ–º –≥—Ä–∞—Ñ–∏–∫–µ –ø—É—Å—Ç–æ (0 –æ—á–µ—Ä–µ–¥–µ–π), –∞ –≤ —Å—Ç–∞—Ä–æ–º –±—ã–ª–æ, –Ω–µ –∑–∞—Ç–∏—Ä–∞–µ–º
        # (–•–æ—Ç—è –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ –æ–±—ã—á–Ω–æ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        if sch['queues'] or sch['date'] not in merged:
            merged[sch['date']] = sch
            
    return list(merged.values())

def main():
    old_schedules = load_existing_schedules()
    log(f"üìÇ –ë—ã–ª–æ –∑–∞–ø–∏—Å–µ–π: {len(old_schedules)}")

    new_found = []
    for url in CHANNELS:
        log(f"üì° {url}")
        res = parse_channel(url)
        if res:
            new_found.extend(res)
        else:
            log("   ‚ö†Ô∏è –ü—É—Å—Ç–æ.")

    final_list = merge_schedules(old_schedules, new_found)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    def date_sorter(item):
        try:
            parts = item['date'].split()
            day = int(parts[0])
            month_str = parts[1]
            month = UA_MONTHS.get(month_str, 0)
            now = datetime.now()
            year = now.year
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ–Ω—ã –≥–æ–¥–∞
            if now.month == 12 and month == 1: year += 1
            elif now.month == 1 and month == 12: year -= 1
            return datetime(year, month, day)
        except:
            return datetime.now()

    final_list.sort(key=date_sorter)
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 5 –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–Ω–µ–π, —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ç—å –º—É—Å–æ—Ä
    final_list = final_list[-5:]

    output_json = {
        "generated_at": get_kiev_time().strftime("%d.%m %H:%M"), 
        "schedules": final_list
    }

    with open('schedule.json', 'w', encoding='utf-8') as f:
        json.dump(output_json, f, ensure_ascii=False, indent=4)
        
    dates_in_file = [item['date'] for item in final_list]
    log(f"üíæ –ò–¢–û–ì: {dates_in_file}")

if __name__ == "__main__":
    main()
