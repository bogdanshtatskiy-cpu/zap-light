import re
import json
import requests
import socket
import sys
import os
import random
import copy
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
import requests.packages.urllib3.util.connection as urllib3_cn

# ==========================
# ğŸ”§ Ğ¤Ğ˜ĞšĞ¡ Ğ”Ğ›Ğ¯ GITHUB ACTIONS (IPv4)
# ==========================
def allowed_gai_family():
    return socket.AF_INET

urllib3_cn.allowed_gai_family = allowed_gai_family

# ==========================
# âš™ï¸ ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ˜
# ==========================
CHANNELS = [
    "https://t.me/s/tvoe_zaporizhzhia",
    "https://t.me/s/zoe_alarm"
]

KEYWORDS = [
    "Ğ“ĞŸĞ’", "Ğ“Ğ ĞĞ¤Ğ†Ğš", "Ğ’Ğ†Ğ”ĞšĞ›Ğ®Ğ§Ğ•Ğ", "Ğ•Ğ›Ğ•ĞšĞ¢Ğ Ğ", "Ğ§Ğ•Ğ Ğ“", 
    "ĞĞĞĞ’Ğ›Ğ•Ğ", "Ğ—ĞœĞ†Ğ", "ĞĞ‘Ğ›Ğ•ĞĞ•Ğ Ğ“Ğ", "Ğ£ĞšĞ Ğ•ĞĞ•Ğ Ğ“Ğ", "Ğ¡Ğ’Ğ†Ğ¢Ğ›"
]

UA_MONTHS = {
    "Ğ¡Ğ†Ğ§ĞĞ¯": 1, "Ğ›Ğ®Ğ¢ĞĞ“Ğ": 2, "Ğ‘Ğ•Ğ Ğ•Ğ—ĞĞ¯": 3, "ĞšĞ’Ğ†Ğ¢ĞĞ¯": 4, "Ğ¢Ğ ĞĞ’ĞĞ¯": 5, "Ğ§Ğ•Ğ Ğ’ĞĞ¯": 6,
    "Ğ›Ğ˜ĞŸĞĞ¯": 7, "Ğ¡Ğ•Ğ ĞŸĞĞ¯": 8, "Ğ’Ğ•Ğ Ğ•Ğ¡ĞĞ¯": 9, "Ğ–ĞĞ’Ğ¢ĞĞ¯": 10, "Ğ›Ğ˜Ğ¡Ğ¢ĞĞŸĞĞ”Ğ": 11, "Ğ“Ğ Ğ£Ğ”ĞĞ¯": 12
}
UA_MONTHS_REVERSE = {v: k for k, v in UA_MONTHS.items()}

NO_OUTAGE_PHRASES = [
    "ĞĞ• Ğ’Ğ˜ĞœĞ˜ĞšĞĞ„Ğ¢Ğ¬Ğ¡Ğ¯", "ĞĞ• Ğ—ĞĞ¡Ğ¢ĞĞ¡ĞĞ’Ğ£Ğ®Ğ¢Ğ¬Ğ¡Ğ¯", "Ğ‘Ğ•Ğ— Ğ’Ğ†Ğ”ĞšĞ›Ğ®Ğ§Ğ•ĞĞ¬", 
    "Ğ¡ĞšĞĞ¡ĞĞ’ĞĞĞ", "Ğ‘Ğ†Ğ›Ğ˜Ğ™", "Ğ—Ğ•Ğ›Ğ•ĞĞ˜Ğ™", "ĞĞ• Ğ’Ğ†Ğ”ĞšĞ›Ğ®Ğ§ĞĞ„Ğ¢Ğ¬Ğ¡Ğ¯"
]

ALL_QUEUES = ["1.1", "1.2", "2.1", "2.2", "3.1", "3.2", "4.1", "4.2", "5.1", "5.2", "6.1", "6.2"]

# ==========================
# ğŸ›  Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜
# ==========================
def get_kiev_time():
    return datetime.now(timezone.utc) + timedelta(hours=2)

def log(msg):
    print(msg)
    sys.stdout.flush()

def get_html(target_url):
    rnd = random.randint(1, 999999)
    urls = [
        target_url,
        f"https://api.allorigins.win/raw?url={quote(target_url)}&rnd={rnd}",
        f"https://api.codetabs.com/v1/proxy?quest={quote(target_url)}&rnd={rnd}",
        f"https://corsproxy.io/?{quote(target_url)}"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }
    
    for i, url in enumerate(urls):
        try:
            if i > 0: log(f"    ğŸ”„ ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞºÑĞ¸ {i}...")
            response = requests.get(url, headers=headers, timeout=5)
            if response.status_code == 200 and "tgme_widget_message_text" in response.text:
                return response.text
        except Exception:
            pass
    return None

def parse_post_date(date_str):
    try:
        dt = datetime.fromisoformat(date_str)
        return dt.astimezone(timezone(timedelta(hours=2)))
    except Exception:
        return get_kiev_time()

def determine_date_from_text(text, post_date):
    text_upper = text.upper()
    months_regex = "|".join(UA_MONTHS.keys())
    date_match = re.search(rf"\b(\d{{1,2}})\s+({months_regex})", text_upper)
    if date_match:
        return f"{int(date_match.group(1))} {date_match.group(2)}"

    header_text = text_upper[:250]
    if re.search(r"\b(ĞĞĞĞ’Ğ›Ğ•ĞĞ|ĞĞĞĞ’Ğ›Ğ•ĞĞĞ¯|ĞĞĞĞ’Ğ˜Ğ›Ğ˜|ĞĞĞĞ’Ğ˜Ğ’|Ğ—ĞœĞ†ĞĞ˜|Ğ—ĞœĞ†ĞĞ•ĞĞ|Ğ¢Ğ•Ğ ĞœĞ†ĞĞĞ’Ğ|Ğ—ĞĞĞ’Ğ£|Ğ¡Ğ¬ĞĞ“ĞĞ”ĞĞ†)\b", header_text):
        return f"{post_date.day} {UA_MONTHS_REVERSE.get(post_date.month, 'Ğ“Ğ Ğ£Ğ”ĞĞ¯')}"

    if "Ğ—ĞĞ’Ğ¢Ğ Ğ" in header_text:
        target_date = post_date + timedelta(days=1)
        return f"{target_date.day} {UA_MONTHS_REVERSE.get(target_date.month, 'Ğ“Ğ Ğ£Ğ”ĞĞ¯')}"

    return f"{post_date.day} {UA_MONTHS_REVERSE.get(post_date.month, 'Ğ“Ğ Ğ£Ğ”ĞĞ¯')}"

def get_date_obj(date_str):
    try:
        parts = date_str.split()
        day = int(parts[0])
        month = UA_MONTHS.get(parts[1], 0)
        now = get_kiev_time()
        year = now.year
        if now.month == 12 and month == 1: year += 1
        elif now.month == 1 and month == 12: year -= 1
        return datetime(year, month, day).date()
    except:
        return get_kiev_time().date()

def time_to_mins(t_str):
    h, m = map(int, t_str.split(':'))
    return h * 60 + m

def mins_to_time(m):
    if m >= 1440: return "24:00"
    return f"{m//60:02d}:{m%60:02d}"

def merge_intervals(intervals):
    if not intervals: return []
    intervals.sort(key=lambda x: time_to_mins(x['start']))
    merged = [intervals[0].copy()]
    for current in intervals[1:]:
        last = merged[-1]
        last_e = time_to_mins(last['end'])
        curr_s = time_to_mins(current['start'])
        curr_e = time_to_mins(current['end'])
        if curr_s <= last_e: 
            new_e = max(last_e, curr_e)
            merged[-1]['end'] = mins_to_time(new_e)
        else:
            merged.append(current.copy())
    return merged

# ==========================
# ğŸ§  ĞŸĞĞ Ğ¡Ğ˜ĞĞ“ ĞšĞĞĞĞ›ĞĞ’
# ==========================
def parse_channel(url):
    html = get_html(url)
    if not html: return []

    soup = BeautifulSoup(html, 'html.parser')
    page_title = soup.title.string.strip() if soup.title else "Channel"
    log(f"    ğŸ” ĞĞ½Ğ°Ğ»Ğ¸Ğ·: {page_title}")
    
    message_wraps = soup.find_all('div', class_='tgme_widget_message')
    if not message_wraps: return []

    found_schedules = []
    time_pattern = re.compile(r"(\d{1,2}[:.;]\d{2})\s*[^\d:.;]+\s*(\d{1,2}[:.;]\d{2})", re.IGNORECASE)
    queue_line_pattern = re.compile(r"^(?:[^\d]{0,20})?((?:\d\.\d\s*(?:[\/,+&]|Ñ–|Ñ‚Ğ°)?\s*)+)(?:\s*[:)])?\s*(.*)", re.IGNORECASE)

    for msg in message_wraps:
        text_div = msg.find('div', class_='tgme_widget_message_text')
        if not text_div: continue
        text = text_div.get_text(separator="\n")

        if not any(k in text.upper() for k in KEYWORDS): continue

        post_date = get_kiev_time()
        time_tag = msg.find('time')
        if time_tag and 'datetime' in time_tag.attrs:
            post_date = parse_post_date(time_tag['datetime'])

        final_date_key = determine_date_from_text(text, post_date)
        if not final_date_key: continue

        updated_at_val = post_date.strftime("%d.%m %H:%M")
        time_upd_match = re.search(r"\(Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾.*(\d{2}:\d{2})\)", text, re.IGNORECASE)
        if time_upd_match:
            updated_at_val = f"{post_date.strftime('%d.%m')} {time_upd_match.group(1)}"

        lines = [line.strip().replace('\xa0', ' ') for line in text.split('\n') if line.strip()]
        queues_found = {}

        for line in lines:
            match = queue_line_pattern.search(line)
            if match:
                queues_part = match.group(1)
                content = match.group(2).lower()
                found_ids = re.findall(r"\d\.\d", queues_part)
                is_no_outage = any(phrase.lower() in content for phrase in NO_OUTAGE_PHRASES)

                intervals = []
                if not is_no_outage:
                    time_matches = list(time_pattern.finditer(content))
                    for tm in time_matches:
                        start, end = tm.groups()
                        start = start.replace('.', ':').replace(';', ':')
                        end = end.replace('.', ':').replace(';', ':')
                        if len(start) == 4: start = "0" + start
                        if len(end) == 4: end = "0" + end
                        intervals.append({"start": start, "end": end})
                
                for q_id in found_ids:
                    if is_no_outage: queues_found[q_id] = []
                    elif intervals: queues_found[q_id] = intervals

        if not queues_found and any(phrase.lower() in text.lower() for phrase in NO_OUTAGE_PHRASES):
            queues_found = {q: [] for q in ALL_QUEUES}

        if queues_found:
            for q_id in queues_found:
                queues_found[q_id] = merge_intervals(queues_found[q_id])

            log(f"    â• ĞĞ°Ğ¹Ğ´ĞµĞ½ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº Ğ½Ğ° {final_date_key} (Ğ¿Ğ¾ÑÑ‚ Ğ¾Ñ‚ {post_date.strftime('%d.%m %H:%M')})")
            found_schedules.append({
                "date": final_date_key,
                "queues": queues_found,
                "updated_at": updated_at_val,
                "_post_timestamp": post_date.timestamp()
            })

    return found_schedules

def load_existing_schedules():
    if os.path.exists('schedule.json'):
        try:
            with open('schedule.json', 'r', encoding='utf-8') as f:
                return json.load(f).get("schedules", [])
        except Exception: return []
    return []

# ==========================
# ğŸ›‘ Ğ›ĞĞ“Ğ˜ĞšĞ: Ğ–Ğ•Ğ¡Ğ¢ĞšĞĞ¯ ĞŸĞ•Ğ Ğ•Ğ—ĞĞŸĞ˜Ğ¡Ğ¬ (Ğ¢ĞĞ›Ğ¬ĞšĞ ĞŸĞĞ›ĞĞ«Ğ• Ğ“Ğ ĞĞ¤Ğ˜ĞšĞ˜)
# ==========================
def merge_schedules(old_data, new_data):
    merged = {sch['date']: copy.deepcopy(sch) for sch in old_data}
    today = get_kiev_time().date()
    
    log("\nğŸ›  Ğ Ğ•Ğ–Ğ˜Ğœ: ĞĞ‘Ğ¡ĞĞ›Ğ®Ğ¢ĞĞĞ¯ ĞŸĞ•Ğ Ğ•Ğ—ĞĞŸĞ˜Ğ¡Ğ¬ (Ğ‘ĞµÑ€ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¾Ñ‚ 10 Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹)...")
    
    by_date = {}
    for sch in new_data:
        d = sch['date']
        if d not in by_date:
            by_date[d] = []
        by_date[d].append(sch)
        
    for date_key, posts in by_date.items():
        d_obj = get_date_obj(date_key)
        
        # Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ´Ğ½ĞµĞ¹
        if d_obj < today:
            continue
            
        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğº ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼ (Ğ¿Ğ¾ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸)
        posts.sort(key=lambda x: x.get('_post_timestamp', 0), reverse=True)
        
        # Ğ˜Ñ‰ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¹ ÑĞ²ĞµĞ¶Ğ¸Ğ¹ ĞŸĞĞ›ĞĞĞ¦Ğ•ĞĞĞ«Ğ™ Ğ¿Ğ¾ÑÑ‚
        best_post = None
        for p in posts:
            q_count = len(p['queues'])
            is_cancel = q_count > 0 and all(len(v) == 0 for v in p['queues'].values())
            # ĞŸĞĞ ĞĞ“ = 10 Ğ¾Ñ‡ĞµÑ€ĞµĞ´ĞµĞ¹. Ğ•ÑĞ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ - ÑÑ‚Ğ¾ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ°Ğ»ĞµÑ€Ñ‚, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµĞ¼!
            if q_count >= 10 or is_cancel:
                best_post = p
                break
                
        # Ğ•ÑĞ»Ğ¸ Ğ·Ğ° Ğ´ĞµĞ½ÑŒ Ğ½Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼
        if not best_post:
            log(f"  â­ ĞŸĞ ĞĞŸĞ£Ğ¡Ğš {date_key}: ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ° (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ»ĞµÑ€Ñ‚Ñ‹).")
            continue
        
        new_ts = best_post.get('_post_timestamp', 0)
        old_ts = merged.get(date_key, {}).get('_post_timestamp', -1)
        
        # Ğ•ÑĞ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº ÑĞ²ĞµĞ¶ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğµ - Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼
        if new_ts >= old_ts:
            for q in ALL_QUEUES:
                if q not in best_post['queues']:
                    best_post['queues'][q] = []

            if date_key not in merged:
                log(f"  âœ¨ Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•Ğ {date_key} (Ğ²Ğ·ÑÑ‚ Ğ¿Ğ¾ÑÑ‚ Ğ¾Ñ‚ {best_post['updated_at']})")
            else:
                log(f"  ğŸ”„ ĞŸĞ•Ğ Ğ•Ğ—ĞĞŸĞ˜Ğ¡ĞĞ {date_key} (Ğ²Ğ·ÑÑ‚ Ğ¿Ğ¾ÑÑ‚ Ğ¾Ñ‚ {best_post['updated_at']})")
            
            merged[date_key] = copy.deepcopy(best_post)
            
    result = []
    for v in merged.values():
        if '_post_timestamp' in v:
            del v['_post_timestamp']
        result.append(v)
        
    return result

def clean_old_schedules(schedules):
    today = get_kiev_time().date()
    cutoff_date = today - timedelta(days=2)
    cleaned = []
    for item in schedules:
        try:
            if get_date_obj(item['date']) >= cutoff_date:
                cleaned.append(item)
        except:
            cleaned.append(item)
    return cleaned

def main():
    old_schedules = load_existing_schedules()
    log(f"ğŸ“‚ Ğ‘Ñ‹Ğ»Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ğ±Ğ°Ğ·Ğµ: {len(old_schedules)}")

    new_found = []
    for url in CHANNELS:
        log(f"ğŸ“¡ {url}")
        res = parse_channel(url)
        if res: new_found.extend(res)
        else: log("    âš ï¸ ĞŸÑƒÑÑ‚Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°.")

    final_list = merge_schedules(old_schedules, new_found)

    final_list.sort(key=lambda x: get_date_obj(x['date']))
    final_list = clean_old_schedules(final_list)[-35:]

    output_json = {
        "generated_at": get_kiev_time().strftime("%d.%m %H:%M"), 
        "schedules": final_list
    }

    with open('schedule.json', 'w', encoding='utf-8') as f:
        json.dump(output_json, f, ensure_ascii=False, indent=4)
        
    dates_in_file = [item['date'] for item in final_list]
    log(f"\nğŸ’¾ Ğ˜Ğ¢ĞĞ“ (Ğ´Ğ½ĞµĞ¹: {len(dates_in_file)}): {dates_in_file}")

if __name__ == "__main__":
    main()
