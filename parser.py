import re
import json
import requests
import socket
import sys
import os
import random
import copy
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
import requests.packages.urllib3.util.connection as urllib3_cn

# ==========================
# üîß –§–ò–ö–° –î–õ–Ø GITHUB ACTIONS (IPv4)
# ==========================
def allowed_gai_family():
    return socket.AF_INET

urllib3_cn.allowed_gai_family = allowed_gai_family

# ==========================
# ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò
# ==========================
CHANNELS = [
    "https://t.me/s/it_is_zp_tg",
    "https://t.me/s/tvoe_zaporizhzhia",
    "https://t.me/s/zapnovini",
    "https://t.me/s/info_zp",
    "https://t.me/s/zoe_alarm"
]

KEYWORDS = [
    "–ì–ü–í", "–ì–†–ê–§–Ü–ö", "–í–Ü–î–ö–õ–Æ–ß–ï–ù", "–ï–õ–ï–ö–¢–†–û", "–ß–ï–†–ì", 
    "–û–ù–û–í–õ–ï–ù", "–ó–ú–Ü–ù", "–û–ë–õ–ï–ù–ï–†–ì–û", "–£–ö–†–ï–ù–ï–†–ì–û", "–°–í–Ü–¢–õ"
]

UA_MONTHS = {
    "–°–Ü–ß–ù–Ø": 1, "–õ–Æ–¢–û–ì–û": 2, "–ë–ï–†–ï–ó–ù–Ø": 3, "–ö–í–Ü–¢–ù–Ø": 4, "–¢–†–ê–í–ù–Ø": 5, "–ß–ï–†–í–ù–Ø": 6,
    "–õ–ò–ü–ù–Ø": 7, "–°–ï–†–ü–ù–Ø": 8, "–í–ï–†–ï–°–ù–Ø": 9, "–ñ–û–í–¢–ù–Ø": 10, "–õ–ò–°–¢–û–ü–ê–î–ê": 11, "–ì–†–£–î–ù–Ø": 12
}
UA_MONTHS_REVERSE = {v: k for k, v in UA_MONTHS.items()}

NO_OUTAGE_PHRASES = [
    "–ù–ï –í–ò–ú–ò–ö–ê–Ñ–¢–¨–°–Ø", "–ù–ï –ó–ê–°–¢–û–°–û–í–£–Æ–¢–¨–°–Ø", "–ë–ï–ó –í–Ü–î–ö–õ–Æ–ß–ï–ù–¨", 
    "–°–ö–ê–°–û–í–ê–ù–û", "–ë–Ü–õ–ò–ô", "–ó–ï–õ–ï–ù–ò–ô", "–ù–ï –í–Ü–î–ö–õ–Æ–ß–ê–Ñ–¢–¨–°–Ø"
]

ALL_QUEUES = ["1.1", "1.2", "2.1", "2.2", "3.1", "3.2", "4.1", "4.2", "5.1", "5.2", "6.1", "6.2"]

# ==========================
# üõ† –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ==========================
def get_kiev_time():
    # –ö–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è (UTC+2)
    return datetime.now(timezone.utc) + timedelta(hours=2)

def log(msg):
    print(msg)
    sys.stdout.flush()

def get_html(target_url):
    rnd = random.randint(1, 999999)
    urls = [
        target_url,
        f"https://api.allorigins.win/raw?url={quote(target_url)}&rnd={rnd}",
        f"https://api.codetabs.com/v1/proxy?quest={quote(target_url)}&rnd={rnd}",
        f"https://corsproxy.io/?{quote(target_url)}"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
    }
    
    for i, url in enumerate(urls):
        try:
            if i > 0: log(f"    üîÑ –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ {i}...")
            response = requests.get(url, headers=headers, timeout=5)
            if response.status_code == 200 and "tgme_widget_message_text" in response.text:
                return response.text
        except Exception:
            pass
    return None

def parse_post_date(date_str):
    try:
        dt = datetime.fromisoformat(date_str)
        return dt.astimezone(timezone(timedelta(hours=2)))
    except Exception:
        return get_kiev_time()

def determine_date_from_text(text, post_date):
    text_upper = text.upper()
    months_regex = "|".join(UA_MONTHS.keys())
    date_match = re.search(rf"\b(\d{{1,2}})\s+({months_regex})", text_upper)
    if date_match:
        return f"{int(date_match.group(1))} {date_match.group(2)}"

    header_text = text_upper[:250]
    if re.search(r"\b(–û–ù–û–í–õ–ï–ù–û|–û–ù–û–í–õ–ï–ù–ù–Ø|–û–ù–û–í–ò–õ–ò|–û–ù–û–í–ò–í|–ó–ú–Ü–ù–ò|–ó–ú–Ü–ù–ï–ù–û|–¢–ï–†–ú–Ü–ù–û–í–û|–ó–ù–û–í–£|–°–¨–û–ì–û–î–ù–Ü)\b", header_text):
        return f"{post_date.day} {UA_MONTHS_REVERSE.get(post_date.month, '–ì–†–£–î–ù–Ø')}"

    if "–ó–ê–í–¢–†–ê" in header_text:
        target_date = post_date + timedelta(days=1)
        return f"{target_date.day} {UA_MONTHS_REVERSE.get(target_date.month, '–ì–†–£–î–ù–Ø')}"

    return f"{post_date.day} {UA_MONTHS_REVERSE.get(post_date.month, '–ì–†–£–î–ù–Ø')}"

def get_date_obj(date_str):
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É '22 –õ–Æ–¢–û–ì–û' –≤ –æ–±—ä–µ–∫—Ç –¥–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å '—Å–µ–≥–æ–¥–Ω—è'"""
    try:
        parts = date_str.split()
        day = int(parts[0])
        month = UA_MONTHS.get(parts[1], 0)
        now = get_kiev_time()
        year = now.year
        if now.month == 12 and month == 1: year += 1
        elif now.month == 1 and month == 12: year -= 1
        return datetime(year, month, day).date()
    except:
        return get_kiev_time().date()

def time_to_mins(t_str):
    h, m = map(int, t_str.split(':'))
    return h * 60 + m

def mins_to_time(m):
    if m >= 1440: return "24:00"
    return f"{m//60:02d}:{m%60:02d}"

def merge_intervals(intervals):
    if not intervals: return []
    intervals.sort(key=lambda x: time_to_mins(x['start']))
    merged = [intervals[0].copy()]
    for current in intervals[1:]:
        last = merged[-1]
        last_e = time_to_mins(last['end'])
        curr_s = time_to_mins(current['start'])
        curr_e = time_to_mins(current['end'])
        if curr_s <= last_e: 
            new_e = max(last_e, curr_e)
            merged[-1]['end'] = mins_to_time(new_e)
        else:
            merged.append(current.copy())
    return merged

# ==========================
# üß† –ü–ê–†–°–ò–ù–ì –ö–ê–ù–ê–õ–û–í
# ==========================
def parse_channel(url):
    html = get_html(url)
    if not html: return []

    soup = BeautifulSoup(html, 'html.parser')
    page_title = soup.title.string.strip() if soup.title else "Channel"
    log(f"    üîé –ê–Ω–∞–ª–∏–∑: {page_title}")
    
    message_wraps = soup.find_all('div', class_='tgme_widget_message')
    if not message_wraps: return []

    found_schedules = []
    time_pattern = re.compile(r"(\d{1,2}[:.;]\d{2})\s*[^\d:.;]+\s*(\d{1,2}[:.;]\d{2})", re.IGNORECASE)
    queue_line_pattern = re.compile(r"^(?:[^\d]{0,20})?((?:\d\.\d\s*(?:[\/,+&]|—ñ|—Ç–∞)?\s*)+)(?:\s*[:)])?\s*(.*)", re.IGNORECASE)

    for msg in message_wraps:
        text_div = msg.find('div', class_='tgme_widget_message_text')
        if not text_div: continue
        text = text_div.get_text(separator="\n")

        if not any(k in text.upper() for k in KEYWORDS): continue

        post_date = get_kiev_time()
        time_tag = msg.find('time')
        if time_tag and 'datetime' in time_tag.attrs:
            post_date = parse_post_date(time_tag['datetime'])

        final_date_key = determine_date_from_text(text, post_date)
        if not final_date_key: continue

        updated_at_val = post_date.strftime("%d.%m %H:%M")
        time_upd_match = re.search(r"\(–æ–Ω–æ–≤–ª–µ–Ω–æ.*(\d{2}:\d{2})\)", text, re.IGNORECASE)
        if time_upd_match:
            updated_at_val = f"{post_date.strftime('%d.%m')} {time_upd_match.group(1)}"

        lines = [line.strip().replace('\xa0', ' ') for line in text.split('\n') if line.strip()]
        queues_found = {}

        for line in lines:
            match = queue_line_pattern.search(line)
            if match:
                queues_part = match.group(1)
                content = match.group(2).lower()
                found_ids = re.findall(r"\d\.\d", queues_part)
                is_no_outage = any(phrase.lower() in content for phrase in NO_OUTAGE_PHRASES)

                intervals = []
                if not is_no_outage:
                    time_matches = list(time_pattern.finditer(content))
                    for tm in time_matches:
                        start, end = tm.groups()
                        start = start.replace('.', ':').replace(';', ':')
                        end = end.replace('.', ':').replace(';', ':')
                        if len(start) == 4: start = "0" + start
                        if len(end) == 4: end = "0" + end
                        intervals.append({"start": start, "end": end})
                
                for q_id in found_ids:
                    if is_no_outage: queues_found[q_id] = []
                    elif intervals: queues_found[q_id] = intervals

        # –ï—Å–ª–∏ –Ω–∞–ø–∏—Å–∞–Ω–æ "–ë–µ–∑ –æ—Ç–∫–ª—é—á–µ–Ω–∏–π –Ω–∞ –≤–µ—Å—å –¥–µ–Ω—å", –æ—á–∏—â–∞–µ–º –≤—Å—ë
        if not queues_found and any(phrase.lower() in text.lower() for phrase in NO_OUTAGE_PHRASES):
            queues_found = {q: [] for q in ALL_QUEUES}

        if queues_found:
            for q_id in queues_found:
                queues_found[q_id] = merge_intervals(queues_found[q_id])

            log(f"    ‚ûï –ù–∞–π–¥–µ–Ω –≥—Ä–∞—Ñ–∏–∫ –Ω–∞ {final_date_key} (–ø–æ—Å—Ç –æ—Ç {post_date.strftime('%d.%m %H:%M')})")
            found_schedules.append({
                "date": final_date_key,
                "queues": queues_found,
                "updated_at": updated_at_val,
                "_post_timestamp": post_date.timestamp()
            })

    return found_schedules

def load_existing_schedules():
    if os.path.exists('schedule.json'):
        try:
            with open('schedule.json', 'r', encoding='utf-8') as f:
                return json.load(f).get("schedules", [])
        except Exception: return []
    return []

# ==========================
# üõë –õ–û–ì–ò–ö–ê: –ñ–ï–°–¢–ö–ê–Ø –ü–ï–†–ï–ó–ê–ü–ò–°–¨ –ü–û–°–õ–ï–î–ù–ò–ú –ü–û–°–¢–û–ú
# ==========================
def merge_schedules(old_data, new_data):
    merged = {sch['date']: copy.deepcopy(sch) for sch in old_data}
    today = get_kiev_time().date()
    
    log("\nüõ† –†–ï–ñ–ò–ú: –ê–ë–°–û–õ–Æ–¢–ù–ê–Ø –ü–ï–†–ï–ó–ê–ü–ò–°–¨ –°–ê–ú–´–ú –°–í–ï–ñ–ò–ú –ü–û–°–¢–û–ú...")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –ø–æ –¥–Ω—è–º
    by_date = {}
    for sch in new_data:
        d = sch['date']
        if d not in by_date:
            by_date[d] = []
        by_date[d].append(sch)
        
    for date_key, posts in by_date.items():
        d_obj = get_date_obj(date_key)
        
        # 1. –ï—Å–ª–∏ —ç—Ç–æ—Ç –¥–µ–Ω—å —É–∂–µ –ø—Ä–æ—à–µ–ª (–≤—á–µ—Ä–∞ –∏–ª–∏ —Ä–∞–Ω—å—à–µ), –º—ã –µ–≥–æ –≤–æ–æ–±—â–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
        if d_obj < today:
            log(f"  ‚è≠ –ü–†–û–ü–£–°–ö {date_key}: –î–µ–Ω—å —É–∂–µ –ø—Ä–æ—à–µ–ª, —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")
            continue
            
        # 2. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã —ç—Ç–æ–≥–æ –¥–Ω—è –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤—Ä–µ–º–µ–Ω–∏ (—Å–∞–º—ã–π —Å–≤–µ–∂–∏–π –≤ –∏–Ω–¥–µ–∫—Å–µ [0])
        posts.sort(key=lambda x: x.get('_post_timestamp', 0), reverse=True)
        best_post = posts[0] 
        
        new_ts = best_post.get('_post_timestamp', 0)
        old_ts = merged.get(date_key, {}).get('_post_timestamp', -1)
        
        if new_ts >= old_ts:
            # 3. –î–æ–±–∞–≤–ª—è–µ–º –ü–£–°–¢–´–ï –º–∞—Å—Å–∏–≤—ã –¥–ª—è —Ç–µ—Ö –æ—á–µ—Ä–µ–¥–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ —É–ø–æ–º—è–Ω—É—Ç—ã –≤ —Å–≤–µ–∂–µ–º –ø–æ—Å—Ç–µ.
            # –≠—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é "—Å—Ç–∏—Ä–∞–µ—Ç" –æ—Å—Ç–∞—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
            for q in ALL_QUEUES:
                if q not in best_post['queues']:
                    best_post['queues'][q] = []

            if date_key not in merged:
                log(f"  ‚ú® –î–û–ë–ê–í–õ–ï–ù –ù–û–í–´–ô –î–ï–ù–¨: {date_key} (–≤–∑—è—Ç –ø–æ—Å—Ç –æ—Ç {best_post['updated_at']})")
            else:
                log(f"  üîÑ –ñ–ï–°–¢–ö–û –ü–ï–†–ï–ó–ê–ü–ò–°–ê–ù: {date_key} (–∑–∞–º–µ–Ω–µ–Ω –ø–æ—Å—Ç–æ–º –æ—Ç {best_post['updated_at']})")
            
            merged[date_key] = copy.deepcopy(best_post)
            
    # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π timestamp –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    result = []
    for v in merged.values():
        if '_post_timestamp' in v:
            del v['_post_timestamp']
        result.append(v)
        
    return result

def clean_old_schedules(schedules):
    today = get_kiev_time().date()
    cutoff_date = today - timedelta(days=2)
    cleaned = []
    for item in schedules:
        try:
            if get_date_obj(item['date']) >= cutoff_date:
                cleaned.append(item)
        except:
            cleaned.append(item)
    return cleaned

def main():
    old_schedules = load_existing_schedules()
    log(f"üìÇ –ë—ã–ª–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {len(old_schedules)}")

    new_found = []
    for url in CHANNELS:
        log(f"üì° {url}")
        res = parse_channel(url)
        if res: new_found.extend(res)
        else: log("    ‚ö†Ô∏è –ü—É—Å—Ç–æ –∏–ª–∏ –Ω–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∞.")

    final_list = merge_schedules(old_schedules, new_found)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–≥–æ JSON –ø–æ –¥–∞—Ç–∞–º
    final_list.sort(key=lambda x: get_date_obj(x['date']))
    final_list = clean_old_schedules(final_list)[-35:]

    output_json = {
        "generated_at": get_kiev_time().strftime("%d.%m %H:%M"), 
        "schedules": final_list
    }

    with open('schedule.json', 'w', encoding='utf-8') as f:
        json.dump(output_json, f, ensure_ascii=False, indent=4)
        
    dates_in_file = [item['date'] for item in final_list]
    log(f"\nüíæ –ò–¢–û–ì (–¥–Ω–µ–π: {len(dates_in_file)}): {dates_in_file}")

if __name__ == "__main__":
    main()
